{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CLASS 1 = BUY OKX \n",
        "CLASS 2 = BUY BINANCE"
      ],
      "metadata": {
        "id": "FwY1JViLjqQ9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtSJV_2G44Wl"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLeVDydiL_8l"
      },
      "outputs": [],
      "source": [
        "!pip install sktime\n",
        "!pip install tabulate\n",
        "from tabulate import tabulate\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import json \n",
        "import websocket\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import datetime as dt\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, CuDNNLSTM\n",
        "from keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from keras.backend import clear_session\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import joblib\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sktime.transformations.panel.rocket import MiniRocketMultivariate\n",
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "import ccxt\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIInNQA5My4f"
      },
      "source": [
        "# **Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fetch data"
      ],
      "metadata": {
        "id": "Q-G4CBu63dJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch OKX DATA\n",
        "exchange = ccxt.okex({\n",
        "    'enableRateLimit': True,\n",
        "    'rateLimit': 10,  \n",
        "})\n",
        "\n",
        "# pair and timeframe\n",
        "symbol = 'BTC/USDT'\n",
        "timeframe = '5m'\n",
        "\n",
        "# start and end date\n",
        "end_date = exchange.milliseconds()  \n",
        "start_date_str = '2019-01-01 00:00:00' \n",
        "\n",
        "# convert string to Unix timestamp milliseconds\n",
        "start_date = int(time.mktime(datetime.datetime.strptime(start_date_str, '%Y-%m-%d %H:%M:%S').timetuple())) * 1000\n",
        "\n",
        "# nrows to fetch per request \n",
        "limit = 100\n",
        "\n",
        "# store data\n",
        "ohlcv_list = []\n",
        "\n",
        "# loop through data in batches of 100 rows\n",
        "while True:\n",
        "    # fetch data with limit of 100 rows per request\n",
        "    ohlcv = exchange.fetch_ohlcv(symbol, timeframe, start_date, params={'to': end_date, 'limit': limit})\n",
        "    \n",
        "    # append\n",
        "    ohlcv_list.extend(ohlcv)\n",
        "    \n",
        "    # update start date for next request\n",
        "    if len(ohlcv) < limit:\n",
        "        break  \n",
        "    else:\n",
        "        start_date = ohlcv[-1][0] + 300000  # add 5 minutes (in milliseconds)\n",
        "\n",
        "# convert to df\n",
        "df = pd.read_json(pd.DataFrame(ohlcv_list).to_json(), orient='records')\n",
        "df.columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
        "\n",
        "# convert timestamp to datetime\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "\n",
        "# timestamp as index\n",
        "df.set_index('timestamp', inplace=True)\n",
        "\n",
        "# save to csv\n",
        "df.to_csv('OKX_historical.csv')\n",
        "\n",
        "# print the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "jyXUs6GF3T8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e-BAlogE4YiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binance Historical"
      ],
      "metadata": {
        "id": "BLULJxkbTFXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "symbol = \"BTCUSDT\"\n",
        "timeframe = \"5m\"\n",
        "start = \"2019-01-01\"\n",
        "end = \"2023-03-01\""
      ],
      "metadata": {
        "id": "lAG3hCFGHxJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "klines = bh.fetch_klines(symbol=symbol, timeframe=timeframe, start=start, end=end)"
      ],
      "metadata": {
        "id": "juL9rpyitW89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "klines = klines.rename(columns = {'open':'open_bnb'})"
      ],
      "metadata": {
        "id": "uZMfrZXV_2z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "klines.to_csv('klines_binance.csv')"
      ],
      "metadata": {
        "id": "NzUXSCaTIDnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n08r2j4TgS3F"
      },
      "source": [
        "## loading and Subsetting "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf55jLZ9MHwM"
      },
      "outputs": [],
      "source": [
        "klines = pd.read_csv('klines_binance.csv', index_col = 'open_datetime', on_bad_lines='warn')\n",
        "print(klines)\n",
        "okx = pd.read_csv('OKX_historical.csv', index_col='timestamp', on_bad_lines='warn')\n",
        "print(okx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRWh78tfMX0c"
      },
      "outputs": [],
      "source": [
        "start_date = pd.Timestamp('2020-10-01', tz='UTC')\n",
        "end_date = pd.Timestamp('2021-03-01', tz='UTC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPPdzIrzMUKW"
      },
      "outputs": [],
      "source": [
        "# Convert to datetime format\n",
        "okx.index = pd.to_datetime(okx.index).tz_localize('UTC')\n",
        "okx = okx.rename(columns={'open':'open_okx'})\n",
        "# Subset \n",
        "okx_subset = okx[(okx.index >= start_date) & (okx.index < end_date)]\n",
        "okx_subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2TEjcnpMUID"
      },
      "outputs": [],
      "source": [
        "# Convert to datetime format\n",
        "klines.index = pd.to_datetime(klines.index).tz_convert('UTC')\n",
        "\n",
        "#subset\n",
        "klines_subset = klines[(klines.index >= start_date) & (klines.index < end_date)]\n",
        "klines_subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzGBYtX5M9pC"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rw4vW8Fqz2M7"
      },
      "outputs": [],
      "source": [
        "# expected number of rows\n",
        "num_intervals = (end_date - start_date) // pd.Timedelta(minutes=5)\n",
        "expected_num_rows = num_intervals + 1\n",
        "\n",
        "print(f\"Expected number of rows: {expected_num_rows}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBHwUozJ5aQ9"
      },
      "outputs": [],
      "source": [
        "# n missing rows\n",
        "date_range = pd.date_range(start_date, end_date, freq='5min')\n",
        "\n",
        "missing_rows_1 = date_range[~date_range.isin(okx_subset.index)]\n",
        "missing_rows_2 = date_range[~date_range.isin(klines_subset.index)]\n",
        "\n",
        "print(f\"Number of missing rows in dataframe 1: {len(missing_rows_1)}\")\n",
        "print(f\"Number of missing rows in dataframe 2: {len(missing_rows_2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOks1icl6o8o"
      },
      "outputs": [],
      "source": [
        "# index of rows in okx_subset not in klines_subset\n",
        "missing_rows = okx_subset[~okx_subset.index.isin(klines_subset.index)].index\n",
        "\n",
        "# Drop missing rows okx_subset\n",
        "okx_subset = okx_subset.drop(missing_rows)\n",
        "okx_subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4r8VMc6MUD1"
      },
      "outputs": [],
      "source": [
        "klines = klines_subset\n",
        "okx = okx_subset \n",
        "print(len(klines))\n",
        "print(len(okx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giVBXEx7NM-B"
      },
      "source": [
        "## Merging "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxBzr-oW97-2"
      },
      "outputs": [],
      "source": [
        "# merge both df\n",
        "merged_df = klines.merge(okx, left_index=True, right_index=True)\n",
        "merged_df = merged_df.drop('close_datetime', axis = 1)\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg2lJbbzOUw-"
      },
      "outputs": [],
      "source": [
        "#renaming columns for clarity\n",
        "merged_df.rename(columns={'high_x':'high_bnb', 'low_x':'low_bnb', 'close_x':'close_bnb', 'volume_x':'volume_bnb', 'trades':'trades_bnb', 'high_y':'high_okx', \n",
        "                  'low_y':'low_okx', 'close_y':'close_okx', 'volume_y':'volume_okx'}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4OGP_PANb7i"
      },
      "source": [
        "## Target variable "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#target variable \n",
        "#buy okx represents the action of buying on okx and selling binance. (Calculated as open_bnb - slip) - (okx + slip)\n",
        "#if positive class_variable = 1\n",
        "#same for bnb\n",
        "merged_df['spread'] = merged_df.open_bnb - merged_df.open_okx\n",
        "merged_df['buy_okx'] = (merged_df.open_bnb - (merged_df.open_bnb*0.0001)) - (merged_df.open_okx + (merged_df.open_okx*0.0001))\n",
        "merged_df['buy_bnb'] = (merged_df.open_okx - (merged_df.open_okx*0.0001)) - (merged_df.open_bnb + (merged_df.open_bnb*0.0001))\n",
        "merged_df['class_variable'] = 0 \n",
        "merged_df.loc[merged_df['buy_okx'] > 0.01, 'class_variable'] = 1\n",
        "merged_df.loc[merged_df['buy_bnb'] > 0.01, 'class_variable'] = 2"
      ],
      "metadata": {
        "id": "Nveqv7wiLrcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "id": "9NJV8M1vNOVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cLNMrRaNkyv"
      },
      "source": [
        "# **Pre-Processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WCzSnoHNvHq"
      },
      "source": [
        "## Timezones "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCLA0135MT_7"
      },
      "outputs": [],
      "source": [
        "merged_df.index = pd.to_datetime(merged_df.index)\n",
        "# tradin zone column\n",
        "merged_df['trading_zone'] = 0\n",
        "\n",
        "# time zone as h of d\n",
        "merged_df.loc[(merged_df.index.hour >= 1) & (merged_df.index.hour < 9), 'trading_zone'] = 'eu'\n",
        "merged_df.loc[(merged_df.index.hour >= 9) & (merged_df.index.hour < 17), 'trading_zone'] = 'na'\n",
        "merged_df.loc[((merged_df.index.hour >= 17) & (merged_df.index.hour <= 23)) | ((merged_df.index.hour >= 0) & (merged_df.index.hour < 1)), 'trading_zone'] = 'as'\n",
        "\n",
        "# encode\n",
        "merged_df = pd.get_dummies(merged_df, columns=['trading_zone'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGvYlgM5N0T9"
      },
      "source": [
        "## Days until/since halving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkVn0gsWr88q"
      },
      "outputs": [],
      "source": [
        "#days until halving\n",
        "merged_df.index = merged_df.index.tz_localize(None)\n",
        "next_halving = dt.datetime(2024, 5, 11)\n",
        "merged_df['days_until_halving'] = (next_halving - merged_df.index).days"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9O-1yZ6uSqQ"
      },
      "outputs": [],
      "source": [
        "#days since halving\n",
        "merged_df.index = merged_df.index.tz_localize(None)\n",
        "halving_dates = [dt.datetime(2016, 7, 9), dt.datetime(2020, 5, 11)]\n",
        "merged_df['days_since_halving'] = [(d - halving_dates[1]).days if d > halving_dates[1] else (d - halving_dates[0]).days for d in merged_df.index]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mryTJ9xiMT6c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tZOpdJTBYYw"
      },
      "source": [
        "## Moving averages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtto8lQvBaoN"
      },
      "outputs": [],
      "source": [
        "def get_sma(df, window, var):\n",
        "    \"\"\"\n",
        "    simple moving avg\n",
        "    expects df, window size and, variable\n",
        "    \"\"\"\n",
        "    return merged_df[var].rolling(window=window).mean()\n",
        "\n",
        "def get_ema(df, window, var):\n",
        "    \"\"\"\n",
        "    exponential moving average\n",
        "    expects df, window size and, variable \n",
        "    \"\"\"\n",
        "    return merged_df[var].ewm(span=window, adjust=False).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vApE1VE8Bjaw"
      },
      "outputs": [],
      "source": [
        "#window size is chosen to capture short, mid and, long term relationships\n",
        "merged_df['SMA20_o'] = get_sma(merged_df, 20, 'open_okx')\n",
        "merged_df['SMA50_o'] = get_sma(merged_df, 50, 'open_okx')\n",
        "merged_df['SMA200_o'] = get_sma(merged_df, 200, 'open_okx')\n",
        "merged_df['EMA12_o'] = get_ema(merged_df, 12, 'open_okx')\n",
        "merged_df['EMA26_o'] = get_ema(merged_df, 26, 'open_okx')\n",
        "merged_df['SMA20_b'] = get_sma(merged_df, 20, 'open_bnb')\n",
        "merged_df['SMA50_b'] = get_sma(merged_df, 50, 'open_bnb')\n",
        "merged_df['SMA200_b'] = get_sma(merged_df, 200, 'open_bnb')\n",
        "merged_df['EMA12_b'] = get_ema(merged_df, 12, 'open_bnb')\n",
        "merged_df['EMA26_b'] = get_ema(merged_df, 26, 'open_bnb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxzCuOtZMT4Z"
      },
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9l2DOQQQOv3"
      },
      "outputs": [],
      "source": [
        "#subset to avoid nan values from moving averages\n",
        "merged_df = merged_df.iloc[199:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zmTAT2eJkKe"
      },
      "source": [
        "## Relative Strength Index (RSI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIxD3-UrKVpq"
      },
      "outputs": [],
      "source": [
        "merged_df.isna().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYJlFvi2Jpg7"
      },
      "outputs": [],
      "source": [
        "def get_rsi(prices, n=14):\n",
        "  \"\"\"\n",
        "  relative strength index\n",
        "  expects variable and window size\n",
        "  \"\"\"\n",
        "    deltas = np.diff(prices)\n",
        "    seed = deltas[:n+1]\n",
        "    up = seed[seed >= 0].sum()/n\n",
        "    down = -seed[seed < 0].sum()/n\n",
        "    rs = up/down\n",
        "    rsi = np.zeros_like(prices)\n",
        "    rsi[:n] = 100. - 100./(1. + rs)\n",
        "\n",
        "    for i in range(n, len(prices)):\n",
        "        delta = deltas[i-1]  \n",
        "        if delta > 0:\n",
        "            upval = delta\n",
        "            downval = 0.\n",
        "        else:\n",
        "            upval = 0.\n",
        "            downval = -delta\n",
        "\n",
        "        up = (up*(n-1) + upval)/n\n",
        "        down = (down*(n-1) + downval)/n\n",
        "\n",
        "        rs = up/down\n",
        "        rsi[i] = 100. - 100./(1. + rs)\n",
        "\n",
        "    return rsi\n",
        "\n",
        "merged_df['rsi_bnb'] = get_rsi(merged_df['open_bnb'])\n",
        "merged_df['rsi_okx'] = get_rsi(merged_df['open_okx'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYfocrJXLAqP"
      },
      "source": [
        "## On Balance Volume (OBV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya92x1EbK_-P"
      },
      "outputs": [],
      "source": [
        "def get_obv(df, col, vol):\n",
        "    \"\"\"\n",
        "    On-Balance Volume\n",
        "    expexcts df, price variable and volume variable\n",
        "    \"\"\"\n",
        "    close_diff = df[col].diff()\n",
        "    obv_direction = pd.Series(np.where(close_diff > 0, 1, np.where(close_diff < 0, -1, 0)), index=df.index)\n",
        "    obv = (obv_direction * df[vol]).cumsum()\n",
        "    \n",
        "    return obv\n",
        "\n",
        "merged_df['obv_bnb'] = get_obv(merged_df, 'open_bnb', 'volume_bnb')\n",
        "merged_df['obv_okx'] = get_obv(merged_df, 'open_okx', 'volume_okx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td-W--4-TDRR"
      },
      "outputs": [],
      "source": [
        "merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3opvB6Iz6iji"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07yCLD3PJNGY"
      },
      "source": [
        "# **X & y**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Osk6YUV8wEKj"
      },
      "outputs": [],
      "source": [
        "#rename for simplicity\n",
        "df = merged_df\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_4K5BX66-ky"
      },
      "outputs": [],
      "source": [
        "#To use if many variables are used\n",
        "\n",
        "# Specify the column name to exclude\n",
        "column_to_exclude = ['class_variable', 'spread','open_okx', 'high_okx', 'low_okx', 'close_okx', 'volume_okx','trading_zone_as', 'trading_zone_eu',\n",
        "       'trading_zone_na', 'days_until_halving', 'days_since_halving', 'trades_bnb', 'quote_volume', 'buy_bnb']\n",
        "\n",
        "# Drop the column and retrieve the remaining column names\n",
        "selected_columns = df.drop(column_to_exclude, axis=1).columns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to float32 to make it run faster (less memory)\n",
        "print(selected_columns)\n",
        "# Specify the variables to convert to float32\n",
        "variables_to_convert = selected_columns\n",
        "\n",
        "# Convert the specified variables to float32\n",
        "df[variables_to_convert] = df[variables_to_convert].astype('float32')\n"
      ],
      "metadata": {
        "id": "bnVZVlury1gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdrprPMjuMyD"
      },
      "outputs": [],
      "source": [
        "# Define the sequence size and features for the rolling windows\n",
        "SEQUENCE_SIZE = 10\n",
        "features = ['open_bnb', 'buy_okx'] \n",
        "#features = ['open_bnb', 'spread_percentage']\n",
        "#features = ['open_bnb', 'high_bnb', 'low_bnb', 'close_bnb', 'volume_bnb',\n",
        "#       'quote_volume', 'trades_bnb', 'spread']\n",
        "#features = ['open_bnb', 'high_bnb', 'low_bnb', 'close_bnb', 'volume_bnb',\n",
        "#       'quote_volume', 'trades_bnb', 'open_okx', 'high_okx', 'low_okx',\n",
        "#      'close_okx', 'volume_okx', 'spread']\n",
        "#features = selected_columns\n",
        "#features = ['open_bnb', 'open_okx', 'obv_bnb', 'obv_okx', 'trading_zone', 'spread']\n",
        "#features = ['open_bnb', 'open_okx', 'spread']\n",
        "#features = ['open_bnb', 'spread']\n",
        "\n",
        "# Array with feature sequence and labels\n",
        "X = np.zeros((len(df) - SEQUENCE_SIZE + 1, SEQUENCE_SIZE, len(features)))\n",
        "y = np.zeros(len(df) - SEQUENCE_SIZE + 1)\n",
        "\n",
        "# Fill arrays\n",
        "for i in range(len(df) - SEQUENCE_SIZE + 1):\n",
        "    X[i, :, :] = df[features].iloc[i:i+SEQUENCE_SIZE].values\n",
        "    y[i] = df['class_variable'].iloc[i+SEQUENCE_SIZE-1]\n",
        "\n",
        "# encode\n",
        "y = to_categorical(y, num_classes=3)\n",
        "\n",
        "# shapes\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcWWm6Mg0I8W"
      },
      "outputs": [],
      "source": [
        "merged_df.class_variable.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **models** "
      ],
      "metadata": {
        "id": "Jmt5jnzo4yN0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfXccf4N7N15"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uCiVFww7M0h"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#model parameters\n",
        "input_shape = (SEQUENCE_SIZE, len(features))\n",
        "activation = \"relu\"\n",
        "loss = 'categorical_crossentropy'\n",
        "learning_rate = 0.001\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "dense_activation = 'softmax'\n",
        "\n",
        "# number of splits for rolling validation\n",
        "n_splits = 5\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# performance metrics\n",
        "classification_reports = []\n",
        "kappa_scores = []\n",
        "\n",
        "# model architecture\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=64, activation='relu', return_sequences=True, input_shape=input_shape, unroll=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=32))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=3, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "# Compile \n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "\n",
        "# scaler\n",
        "scaler = MinMaxScaler((-1,1))\n",
        "\n",
        "# scale at each split and not before to avoid data leakage\n",
        "\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    # Train and Test for current split \n",
        "    X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
        "    y_train_split, y_test_split = y[train_index], y[test_index]\n",
        "\n",
        "    # Reshape for scaling\n",
        "    X_train_fold_reshaped = X_train_fold.reshape(X_train_fold.shape[0], -1)\n",
        "    X_test_fold_reshaped = X_test_fold.reshape(X_test_fold.shape[0], -1)\n",
        "\n",
        "    # fit_transform only on train, transform on test to avoid leakage\n",
        "    X_train_scaled = scaler.fit_transform(X_train_fold_reshaped)\n",
        "    X_test_scaled = scaler.transform(X_test_fold_reshaped)\n",
        "    #reshape back for LSTM\n",
        "    X_train_scaled = X_train_scaled.reshape(X_train_fold.shape)\n",
        "    X_test_scaled = X_test_scaled.reshape(X_test_fold.shape)\n",
        "\n",
        "\n",
        "    # early stopping to limit training time and avoid overfitting\n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "\n",
        "    # Fit on training data\n",
        "    history = model.fit(X_train_scaled, y_train_split, epochs=50, batch_size=32, callbacks=[es], validation_split=0.2, verbose=1)\n",
        "\n",
        "    # predictions on test \n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "    # Convert to labels for profitability and metrics\n",
        "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "    y_test_labels = np.argmax(y_test_split, axis=1)\n",
        "\n",
        "    # Classification report and Kappa for split\n",
        "    report = classification_report(y_test_labels, y_pred_labels, output_dict=True)\n",
        "    kappa = cohen_kappa_score(y_test_labels, y_pred_labels)\n",
        "\n",
        "    # store metrics\n",
        "    classification_reports.append(report)\n",
        "    kappa_scores.append(kappa)\n",
        "\n",
        "    # Print classification report and kappa for split\n",
        "    print(\"------\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "    print(\"Cohen's Kappa:\", kappa)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "XCBJaSBeisrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#average classification report and Kappa score\n",
        "\n",
        "# store metrics\n",
        "class_metrics = {}\n",
        "\n",
        "# Process individual reports\n",
        "for i, report in enumerate(classification_reports):\n",
        "    print(f\"Fold {i+1}:\")\n",
        "    table = []\n",
        "    headers = ['Class', 'Precision', 'Recall', 'F1-Score', 'Support']\n",
        "    table.append(headers)\n",
        "    for class_label, metrics in report.items():\n",
        "        if class_label.isnumeric():\n",
        "            row = [class_label, metrics['precision'], metrics['recall'], metrics['f1-score'], metrics['support']]\n",
        "            table.append(row)\n",
        "            if class_label not in class_metrics:\n",
        "                class_metrics[class_label] = {\n",
        "                    'precisions': [],\n",
        "                    'recalls': [],\n",
        "                    'f1_scores': [],\n",
        "                    'supports': []\n",
        "                }\n",
        "            class_metrics[class_label]['precisions'].append(metrics['precision'])\n",
        "            class_metrics[class_label]['recalls'].append(metrics['recall'])\n",
        "            class_metrics[class_label]['f1_scores'].append(metrics['f1-score'])\n",
        "            class_metrics[class_label]['supports'].append(metrics['support'])\n",
        "    print(tabulate(table, headers='firstrow'))\n",
        "    print(\"\")\n",
        "\n",
        "    # Print Kappa\n",
        "    kappa_score = kappa_scores[i]\n",
        "    print(f\"Cohen's Kappa Score: {kappa_score}\")\n",
        "    print(\"------\")\n",
        "\n",
        "# average metrics for each class\n",
        "avg_report = {}\n",
        "avg_table = []\n",
        "avg_headers = ['Class', 'Precision', 'Recall', 'F1-Score', 'Support']\n",
        "avg_table.append(avg_headers)\n",
        "for class_label, metrics in class_metrics.items():\n",
        "    avg_precision = np.mean(metrics['precisions'])\n",
        "    avg_recall = np.mean(metrics['recalls'])\n",
        "    avg_f1_score = np.mean(metrics['f1_scores'])\n",
        "    avg_support = np.sum(metrics['supports'])\n",
        "    avg_report[class_label] = {\n",
        "        'precision': avg_precision,\n",
        "        'recall': avg_recall,\n",
        "        'f1-score': avg_f1_score,\n",
        "        'support': avg_support\n",
        "    }\n",
        "    avg_row = [class_label, avg_precision, avg_recall, avg_f1_score, avg_support]\n",
        "    avg_table.append(avg_row)\n",
        "\n",
        "# Print average report\n",
        "print(\"Average Report:\")\n",
        "print(tabulate(avg_table, headers='firstrow'))\n",
        "\n",
        "# average Cohen's Kappa score\n",
        "avg_kappa_score = np.mean(kappa_scores)\n",
        "\n",
        "# average Cohen's Kappa score\n",
        "print(f\"Average Cohen's Kappa Score: {avg_kappa_score}\")\n"
      ],
      "metadata": {
        "id": "Mc_Jx_3Lm8m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kappa"
      ],
      "metadata": {
        "id": "Ayk-h2CLtYjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save model\n",
        "joblib.dump(model, 'Chopp_Lstm_ALLv.joblib')"
      ],
      "metadata": {
        "id": "Krm_YkdRWu4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF0EF---ETcb"
      },
      "source": [
        "## MiniRocketMultivariate + LGBM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.class_variable.value_counts()\n"
      ],
      "metadata": {
        "id": "0iJoDqRnVuyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in merged_df.class_variable.value_counts():\n",
        "  print(i/len(merged_df.class_variable))\n"
      ],
      "metadata": {
        "id": "z2-kmc_0V8aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SVpMsukEQMG"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# y back to 1d because MiniRocket expects it\n",
        "y_1d = np.argmax(y, axis=1)\n",
        "\n",
        "# swap n_features and dsequence length because MiniRocket expects it\n",
        "X_swapped = np.swapaxes(X, 1, 2)\n",
        "\n",
        "\n",
        "# pipeline MiniRocketMultivariate and LGBM classifier\n",
        "MiniLgbm = make_pipeline(\n",
        "    MiniRocketMultivariate(num_kernels=10000, max_dilations_per_kernel = 32, n_jobs = -1, random_state = 7),\n",
        "    LGBMClassifier(objective='multiclass', n_estimators = 250, random_state = 7)\n",
        ")\n",
        "\n",
        "# number of splits for rolling validation\n",
        "n_splits = 5\n",
        "\n",
        "# n splits\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "# metrics\n",
        "classification_reports = []\n",
        "cohen_kappa_scores = []\n",
        "\n",
        "# rolling window validation for robust results\n",
        "for train_index, test_index in tscv.split(X_swapped):\n",
        "    # train and test for current split\n",
        "    X_train, X_test = X_swapped[train_index], X_swapped[test_index]\n",
        "    y_train, y_test = y_1d[train_index], y_1d[test_index]\n",
        "\n",
        "    #scale at each split to avoid data leakage\n",
        "    scaler = MinMaxScaler((-1,1))\n",
        "\n",
        "    #reshape for scaling\n",
        "    X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
        "    X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "    #fit tranform only train, transform only test to avoid data leakage\n",
        "    X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
        "    X_test_scaled = scaler.transform(X_test_reshaped)\n",
        "\n",
        "    #reshape for MiniRocket\n",
        "    X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
        "    X_test_scaled = X_test_scaled.reshape(X_test.shape)\n",
        "\n",
        "    # Fit pipeline on train \n",
        "    MiniLgbm.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # predictions on test\n",
        "    y_pred = MiniLgbm.predict(X_test_scaled)\n",
        "\n",
        "    # store reports \n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    classification_reports.append(report)\n",
        "\n",
        "    # kappa score for current split \n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "    cohen_kappa_scores.append(kappa)\n",
        "\n",
        "    print(\"------\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "    print(f\"Cohen's Kappa Score: {kappa}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All reports + Average classification report and Cohen Kappa score\n",
        "class_metrics = {}\n",
        "\n",
        "# individual reports\n",
        "for i, report in enumerate(classification_reports):\n",
        "    print(f\"Fold {i+1}:\")\n",
        "    table = []\n",
        "    headers = ['Class', 'Precision', 'Recall', 'F1-Score', 'Support']\n",
        "    table.append(headers)\n",
        "    for class_label, metrics in report.items():\n",
        "        if class_label.isnumeric():\n",
        "            row = [class_label, metrics['precision'], metrics['recall'], metrics['f1-score'], metrics['support']]\n",
        "            table.append(row)\n",
        "            if class_label not in class_metrics:\n",
        "                class_metrics[class_label] = {\n",
        "                    'precisions': [],\n",
        "                    'recalls': [],\n",
        "                    'f1_scores': [],\n",
        "                    'supports': []\n",
        "                }\n",
        "            class_metrics[class_label]['precisions'].append(metrics['precision'])\n",
        "            class_metrics[class_label]['recalls'].append(metrics['recall'])\n",
        "            class_metrics[class_label]['f1_scores'].append(metrics['f1-score'])\n",
        "            class_metrics[class_label]['supports'].append(metrics['support'])\n",
        "    print(tabulate(table, headers='firstrow'))\n",
        "    print(\"\")\n",
        "\n",
        "    # Cohen's Kappa Score\n",
        "    kappa_score = cohen_kappa_scores[i]\n",
        "    print(f\"Cohen's Kappa Score: {kappa_score}\")\n",
        "    print(\"------\")\n",
        "\n",
        "# average metrics for each class\n",
        "avg_report = {}\n",
        "avg_table = []\n",
        "avg_headers = ['Class', 'Precision', 'Recall', 'F1-Score', 'Support']\n",
        "avg_table.append(avg_headers)\n",
        "for class_label, metrics in class_metrics.items():\n",
        "    avg_precision = np.mean(metrics['precisions'])\n",
        "    avg_recall = np.mean(metrics['recalls'])\n",
        "    avg_f1_score = np.mean(metrics['f1_scores'])\n",
        "    avg_support = np.sum(metrics['supports'])\n",
        "    avg_report[class_label] = {\n",
        "        'precision': avg_precision,\n",
        "        'recall': avg_recall,\n",
        "        'f1-score': avg_f1_score,\n",
        "        'support': avg_support\n",
        "    }\n",
        "    avg_row = [class_label, avg_precision, avg_recall, avg_f1_score, avg_support]\n",
        "    avg_table.append(avg_row)\n",
        "\n",
        "# average report\n",
        "print(\"Average Report:\")\n",
        "print(tabulate(avg_table, headers='firstrow'))\n",
        "\n",
        "#average Cohen's Kappa score\n",
        "avg_kappa_score = np.mean(cohen_kappa_scores)\n",
        "\n",
        "#average Cohen's Kappa score\n",
        "print(f\"Average Cohen's Kappa Score: {avg_kappa_score}\")\n"
      ],
      "metadata": {
        "id": "FHS6iWDPjW5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(MiniLgbm, 'Chopp_Mini_ALLv.joblib')"
      ],
      "metadata": {
        "id": "iI9oGYOO6wIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = joblib.load('mini_4m_2V_chop.joblib')"
      ],
      "metadata": {
        "id": "yRpNffc7NKPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape, y_pred.shape"
      ],
      "metadata": {
        "id": "BOnA8-9NPf3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAewyn0OgowG"
      },
      "source": [
        "### MiniRocketMultivariate + GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfHGIwId-yoS"
      },
      "outputs": [],
      "source": [
        "y_train_1d = np.argmax(y_train, axis=1)\n",
        "y_val_1d = np.argmax(y_val, axis=1)\n",
        "y_test_1d = np.argmax(y_test, axis=1)\n",
        "\n",
        "# pipeline minirocket + gradientboostingclassifier\n",
        "MiniGrad = make_pipeline(\n",
        "    MiniRocketMultivariate(),\n",
        "    GradientBoostingClassifier(random_state=42)\n",
        ")\n",
        "MiniGrad.fit(X_train_scaled_swapped, y_train_1d)\n",
        "MiniGrad.score(X_val_scaled_swapped, y_val_1d)\n",
        "MiniGrad.score(X_test_scaled_swapped, y_test_1d)\n",
        "\n",
        "y_pred = MiniGrad.predict(X_test_scaled_swapped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhspU0igFlKF"
      },
      "source": [
        "### GridSearch MiniGrad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysB6XhkC0b_n"
      },
      "outputs": [],
      "source": [
        "#grid search to find optimal parameters\n",
        "y_train_1d = np.argmax(y_train, axis=1)\n",
        "\n",
        "\n",
        "# pipeline\n",
        "minirocket = make_pipeline(\n",
        "    MiniRocketMultivariate(),\n",
        "    GradientBoostingClassifier(random_state=42)\n",
        ")\n",
        "\n",
        "# hyperparameter grid for GradientBoostingClassifier\n",
        "param_grid = {\n",
        "    'gradientboostingclassifier__learning_rate': [0.1, 0.01, 0.001],\n",
        "    'gradientboostingclassifier__n_estimators': [100, 200, 300],\n",
        "    'gradientboostingclassifier__max_depth': [3, 4, 5]\n",
        "}\n",
        "\n",
        "# Perform grid search for hyperparameter tuning\n",
        "grid_search = GridSearchCV(minirocket, param_grid, cv=5)\n",
        "grid_search.fit(X_train_scaled_swapped, y_train_1d)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best Parameters: \", grid_search.best_params_)\n",
        "print(\"Best Score: \", grid_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlQ0WDzwFpvh"
      },
      "source": [
        "### MiniRocketMultivariate + RidgeClassifierCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdtZMSkemBRJ"
      },
      "outputs": [],
      "source": [
        "# Define the MiniRocket pipeline with Ridge regression\n",
        "MiniRidge = make_pipeline(\n",
        "    MiniRocketMultivariate(),\n",
        "    RidgeClassifierCV()\n",
        ")\n",
        "MiniRidge.fit(X_train_scaled_swapped, y_train)\n",
        "MiniRidge.score(X_val_scaled_swapped, y_val)\n",
        "MiniRidge.score(X_test_scaled_swapped, y_test)\n",
        "# make predictions on test data\n",
        "y_pred = MiniRidge.predict(X_test_scaled_swapped)\n",
        "\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('Precision:', precision_score(y_test, y_pred, average='weighted'))\n",
        "print('Recall:', recall_score(y_test, y_pred, average='weighted'))\n",
        "print('F1-score:', f1_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "hamming_loss_value = hamming_loss(y_test, y_pred)\n",
        "print(\"Hamming Loss:\", hamming_loss_value)\n",
        "\n",
        "jaccard_scores = jaccard_score(y_test, y_pred, average=None)\n",
        "print(\"Jaccard Similarity Scores:\", jaccard_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTV2-Nc5iiVt"
      },
      "outputs": [],
      "source": [
        "# make predictions on test data\n",
        "y_pred = minirocket.predict(X_test_scaled_swapped)\n",
        "\n",
        "print('Accuracy:', accuracy_score(y_test_1d, y_pred))\n",
        "print('Precision:', precision_score(y_test_1d, y_pred, average='weighted'))\n",
        "print('Recall:', recall_score(y_test_1d, y_pred, average='weighted'))\n",
        "print('F1-score:', f1_score(y_test_1d, y_pred, average='weighted'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOADING"
      ],
      "metadata": {
        "id": "fp7MIyiyNjBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load model\n",
        "loaded_model = joblib.load('Chop_Mini_2v.joblib')"
      ],
      "metadata": {
        "id": "i84fcZzL0Up0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#swap for MiniRocket\n",
        "y_1d = np.argmax(y, axis=1)\n",
        "X_swapped = np.swapaxes(X, 1, 2)"
      ],
      "metadata": {
        "id": "aWGjgZZGNhWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split \n",
        "size = 0.2 \n",
        "X_train, X_test, y_train, y_test = train_test_split(X_swapped, y_1d, test_size=size, shuffle = False)"
      ],
      "metadata": {
        "id": "ae7Fqqi6OcuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reshape, scale, reshape\n",
        "scaler = MinMaxScaler((-1,1))\n",
        "\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
        "X_test_scaled = scaler.transform(X_test_reshaped)\n",
        "\n",
        "X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
        "X_test_scaled = X_test_scaled.reshape(X_test.shape)"
      ],
      "metadata": {
        "id": "4tY9ZjIUO91I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict with saved model\n",
        "y_pred = loaded_model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "UYa2_w5KOuyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kappa score\n",
        "cohen_kappa_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "Flo0_l5pQdTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#report\n",
        "classification_report(y_test, y_pred, output_dict=True)"
      ],
      "metadata": {
        "id": "c1qn8e_GPw1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysis**"
      ],
      "metadata": {
        "id": "5yfIe3ev5BKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## chains"
      ],
      "metadata": {
        "id": "qWJ-mzPlDf95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#find the longest chain of 1\n",
        "def longest_chain_of_ones(class_variable):\n",
        "  \"\"\"\n",
        "  expects variable\n",
        "  \"\"\"\n",
        "    current_chain = 0\n",
        "    max_chain = 0\n",
        "    \n",
        "    for value in class_variable:\n",
        "        if value == 1:\n",
        "            current_chain += 1\n",
        "            max_chain = max(max_chain, current_chain)\n",
        "        else:\n",
        "            current_chain = 0\n",
        "    \n",
        "    return max_chain\n",
        "longest_chain = longest_chain_of_ones(merged_df.class_variable)\n",
        "print(\"Longest chain of 1s:\", longest_chain)\n"
      ],
      "metadata": {
        "id": "8Ia4J6dmBBUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#subsets the longest chain of 1\n",
        "def subset_dataframe(df, column_name):\n",
        "    longest_chain = 0\n",
        "    current_chain = 0\n",
        "    start_index = 0\n",
        "    end_index = 0\n",
        "    \n",
        "    for i, value in enumerate(df[column_name]):\n",
        "        if value == 1:\n",
        "            current_chain += 1\n",
        "            if current_chain > longest_chain:\n",
        "                longest_chain = current_chain\n",
        "                start_index = i - longest_chain + 1\n",
        "                end_index = i + 1\n",
        "        else:\n",
        "            current_chain = 0\n",
        "    \n",
        "    subset = df.iloc[start_index:end_index]\n",
        "    return subset\n",
        "\n",
        "\n",
        "subset = subset_dataframe(merged_df, 'class_variable')\n",
        "subset = pd.DataFrame(subset)\n",
        "subset.head(50)"
      ],
      "metadata": {
        "id": "lvp83OOjCZco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#subset longest chain of 2\n",
        "def subset_dataframe(df, column_name):\n",
        "  \"\"\"\n",
        "  expects df and column name \n",
        "  \"\"\"\n",
        "    longest_chain = 0\n",
        "    current_chain = 0\n",
        "    start_index = 0\n",
        "    end_index = 0\n",
        "    \n",
        "    for i, value in enumerate(df[column_name]):\n",
        "        if value == 2:\n",
        "            current_chain += 1\n",
        "            if current_chain > longest_chain:\n",
        "                longest_chain = current_chain\n",
        "                start_index = i - longest_chain + 1\n",
        "                end_index = i + 1\n",
        "        else:\n",
        "            current_chain = 0\n",
        "    \n",
        "    subset = df.iloc[start_index:end_index]\n",
        "    return subset\n",
        "\n",
        "\n",
        "subset = subset_dataframe(merged_df, 'class_variable')\n",
        "subset = pd.DataFrame(subset)\n",
        "subset.tail(50)\n"
      ],
      "metadata": {
        "id": "HH7y-j6ECwBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of chains longer than min_length\n",
        "def count_long_chains(df, column_name, min_length):\n",
        "  \"\"\"\n",
        "  expects df, column name and min length\n",
        "  \"\"\"\n",
        "  \n",
        "    num_long_chains = 0\n",
        "    current_chain = 0\n",
        "    \n",
        "    for value in df[column_name]:\n",
        "        if value == 1:\n",
        "            current_chain += 1\n",
        "            if current_chain > min_length:\n",
        "                num_long_chains += 1\n",
        "        else:\n",
        "            current_chain = 0\n",
        "    \n",
        "    return num_long_chains\n",
        "\n",
        "\n",
        "min_length = 5\n",
        "num_long_chains = count_long_chains(merged_df, 'class_variable', min_length)\n",
        "print(\"Number of chains longer than\", min_length, \":\", num_long_chains)\n"
      ],
      "metadata": {
        "id": "fXNgcLe5Sovh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot chains class 1\n",
        "def plot_chain_length_distribution(class_variable):\n",
        "  \"\"\"\n",
        "  expects variable\n",
        "  \"\"\"\n",
        "    chain_lengths = []\n",
        "    current_chain = 0\n",
        "    non_chain_count = 0\n",
        "\n",
        "    for value in class_variable:\n",
        "        if value == 1:\n",
        "            current_chain += 1\n",
        "        else:\n",
        "            if current_chain > 0:\n",
        "                chain_lengths.append(current_chain)\n",
        "                current_chain = 0\n",
        "            non_chain_count += 1\n",
        "\n",
        "    if current_chain > 0:\n",
        "        chain_lengths.append(current_chain)\n",
        "\n",
        "    chain_lengths.append(0) \n",
        "    non_chain_count += 1\n",
        "\n",
        "    plt.hist(chain_lengths, bins=max(chain_lengths)+1)\n",
        "    plt.xlabel('Chain Length')\n",
        "    plt.ylabel('Number of Occurrences')\n",
        "    plt.xticks(range(max(chain_lengths)+1))\n",
        "    plt.title('Chain Length Distribution')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_chain_length_distribution(merged_df.class_variable)\n"
      ],
      "metadata": {
        "id": "XopupUrMTZsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot chains class 2\n",
        "def plot_chain_length_distribution(class_variable):\n",
        "  \"\"\"\n",
        "  expects variable\n",
        "  \"\"\"\n",
        "    chain_lengths = []\n",
        "    current_chain = 0\n",
        "    non_chain_count = 0\n",
        "\n",
        "    for value in class_variable:\n",
        "        if value == 2:\n",
        "            current_chain += 1\n",
        "        else:\n",
        "            if current_chain > 0:\n",
        "                chain_lengths.append(current_chain)\n",
        "                current_chain = 0\n",
        "            non_chain_count += 1\n",
        "\n",
        "    if current_chain > 0:\n",
        "        chain_lengths.append(current_chain)\n",
        "\n",
        "    chain_lengths.append(0)  \n",
        "    non_chain_count += 1\n",
        "\n",
        "    max_chain_length = max(chain_lengths)\n",
        "    bins = range(max_chain_length + 1)\n",
        "\n",
        "    plt.hist(chain_lengths, bins=max(chain_lengths)+1)\n",
        "    plt.xlabel('Chain Length')\n",
        "    plt.ylabel('Number of Occurrences')\n",
        "    plt.xticks(range(0, max_chain_length + 1, 3))\n",
        "    plt.title('Chain Length Distribution')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_chain_length_distribution(merged_df.class_variable)\n"
      ],
      "metadata": {
        "id": "nFK47B-UYP-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.class_variable.value_counts()"
      ],
      "metadata": {
        "id": "a-ve4b-5YVne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## transitions"
      ],
      "metadata": {
        "id": "HeSz1NTSDmRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#average number of class 0 instance between class -1 and 2\n",
        "def calculate_average_zeros_between(df, column_name):\n",
        "  \"\"\"\n",
        "  expects df and column_name\n",
        "  \"\"\"\n",
        "  \n",
        "    count_zeros = 0\n",
        "    count_transitions = 0\n",
        "    between_zeros = []\n",
        "\n",
        "    for value in df[column_name]:\n",
        "        #print(\"Value:\", value)\n",
        "        if value == 1:\n",
        "            if count_transitions > 0:\n",
        "                between_zeros.append(count_zeros)\n",
        "                count_zeros = 0\n",
        "            count_transitions += 1\n",
        "        elif value == 0:\n",
        "            count_zeros += 1\n",
        "        elif value == 2:\n",
        "            if count_zeros > 0:\n",
        "                between_zeros.append(count_zeros)\n",
        "                count_zeros = 0\n",
        "\n",
        "    if count_transitions > 0:\n",
        "        average_zeros_between = sum(between_zeros) / count_transitions\n",
        "    else:\n",
        "        average_zeros_between = 0\n",
        "\n",
        "    return average_zeros_between\n",
        "\n",
        "\n",
        "# Apply on merged_df DataFrame\n",
        "average_zeros_between = calculate_average_zeros_between(merged_df, 'class_variable')\n",
        "print(\"Average number of class 0 between instances of 1 and 2:\", average_zeros_between)\n"
      ],
      "metadata": {
        "id": "ljEOnWfUrmoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Profitability\n",
        "Althoug there are more opportunities in class 2, they are more profitable in class 1 with respect to how the code works. "
      ],
      "metadata": {
        "id": "QapJuaLLDv0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.argmax(y_pred, axis=1)"
      ],
      "metadata": {
        "id": "jcU0xwXAwNYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the profitability df\n",
        "prof_df = merged_df.tail(len(y_test))\n",
        "prof_df['y_test'] = y_test\n",
        "prof_df['y_pred'] = y_pred\n",
        "prof_df = prof_df[['buy_okx', 'buy_bnb', 'y_test', 'y_pred']]\n",
        "prof_df = prof_df.reset_index()"
      ],
      "metadata": {
        "id": "Wlm5E5cB3BXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#quick overview of what profitability to expect\n",
        "print(prof_df.y_pred.value_counts())\n",
        "print(prof_df.y_test.value_counts())"
      ],
      "metadata": {
        "id": "xKQqSmO0D8vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### class1"
      ],
      "metadata": {
        "id": "rwXPjn4KD_n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prof_df"
      ],
      "metadata": {
        "id": "M9QMa50dDItI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to array\n",
        "buy_bnb = np.array(prof_df.buy_bnb)\n",
        "buy_okx = np.array(prof_df.buy_okx)\n",
        "y_pred = np.array(prof_df.y_pred)\n",
        "\n",
        "#list of closest values, aka values the spread at which you leave the trade\n",
        "closest_values1 = []\n",
        "\n",
        "for i in range(len(buy_okx)):\n",
        "    current_okx = buy_okx[i]\n",
        "    \n",
        "    if y_pred[i] == 1 and current_okx < 0:\n",
        "      if i + 20 < len(buy_bnb):\n",
        "        closest_value = abs(buy_bnb[i + 20])\n",
        "      else:\n",
        "        closest_value = abs(buy_bnb[i+1])\n",
        "\n",
        "    else:\n",
        "        if y_pred[i] == 1:\n",
        "            bnb_values_after_okx = abs(buy_bnb[i + 1:])\n",
        "\n",
        "            # closest value in bnb_values_after_okx that is smaller than current_okx\n",
        "            valid_values = bnb_values_after_okx[abs(bnb_values_after_okx) < current_okx]\n",
        "            if len(valid_values) > 0:\n",
        "                closest_value = abs(valid_values[0])\n",
        "            else:\n",
        "                closest_value = abs(buy_bnb[i+1])\n",
        "        else:\n",
        "            closest_value = np.nan\n",
        "    \n",
        "    closest_values1.append(closest_value)\n",
        "\n",
        "print(closest_values1)\n"
      ],
      "metadata": {
        "id": "Pkog5xggjeVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create and print the lists\n",
        "sells_bnb = [value for value in closest_values1 if not np.isnan(value)]\n",
        "print(len(sells_bnb))\n",
        "buys_okx = prof_df[prof_df.y_pred == 1]['buy_okx']\n",
        "print(len(buys_okx))\n",
        "#profit class 1 calculated by subtracting all exit spreads from entry spreads\n",
        "profit_class1 = [x - y for x, y in zip(buys_okx, sells_bnb)]\n",
        "print(sum(profit_class1))"
      ],
      "metadata": {
        "id": "Uv1jqc8PxjvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#average of negative values to investigate profitability\n",
        "neg = []\n",
        "for i in profit_class1:\n",
        "  if i < 0:\n",
        "    neg.append(i)\n",
        "\n",
        "sum(neg)/len(neg)"
      ],
      "metadata": {
        "id": "0Dd84R63GK-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### class2"
      ],
      "metadata": {
        "id": "7wL_mBAnD9V5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# arrays\n",
        "buy_bnb = np.array(prof_df.buy_bnb)\n",
        "buy_okx = np.array(prof_df.buy_okx)\n",
        "y_pred = np.array(prof_df.y_pred)\n",
        "\n",
        "#list of exit spreads\n",
        "closest_values2 = []\n",
        "for i in range(len(buy_bnb)):\n",
        "    current_bnb = buy_bnb[i]\n",
        "    \n",
        "    if y_pred[i] == 2 and current_bnb < 0:\n",
        "      if i + 20 < len(buy_okx):\n",
        "        closest_value = abs(buy_okx[i + 20])\n",
        "      else:\n",
        "        closest_value = abs(buy_okx[i+1])\n",
        "\n",
        "    else:\n",
        "        if y_pred[i] == 2:\n",
        "            okx_values_after_bnb = abs(buy_okx[i + 1:])\n",
        "\n",
        "            #closest value in okx_values_after_bnb that is smaller than current_bnb\n",
        "            valid_values = okx_values_after_bnb[abs(okx_values_after_bnb) < current_bnb]\n",
        "            if len(valid_values) > 0:\n",
        "                closest_value = abs(valid_values[0])\n",
        "            else:\n",
        "                closest_value = abs(buy_okx[i+1])\n",
        "        else:\n",
        "            closest_value = np.nan\n",
        "    closest_values2.append(closest_value)\n",
        "print(closest_values2)\n"
      ],
      "metadata": {
        "id": "YgsDn0Qb8Pw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#entry and exit spreads\n",
        "sells_okx = [value for value in closest_values2 if not np.isnan(value)]\n",
        "print(len(sells_okx))\n",
        "buys_bnb = prof_df[prof_df.y_pred == 2]['buy_bnb']\n",
        "print(len(buys_bnb))\n",
        "#profit class2 which is calculated as entry spreads - exit spreads\n",
        "profit_class2 = [x - y for x, y in zip(buys_bnb, sells_okx)]\n",
        "print(sum(profit_class2))"
      ],
      "metadata": {
        "id": "tHCld7TPLTk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#investigate negative values to understand profitability better\n",
        "buys_bnb = np.array(buys_bnb)  # Assuming buys_bnb is the array\n",
        "sells_okx = [value for value in sells_okx]  # Assuming sells_okx is the list\n",
        "\n",
        "result = buys_bnb - sells_okx\n",
        "\n",
        "neg = []\n",
        "for i in result:\n",
        "  if i < 0:\n",
        "    neg.append(i)\n",
        "sum(neg)/len(neg)"
      ],
      "metadata": {
        "id": "_L45YNshF1ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize model performance"
      ],
      "metadata": {
        "id": "r2cFo0wrBL4a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EbWMhsMhJWIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cohen kappa for each model at each fold for each period\n",
        "LSTM_bear = [0.95, 0, 0.89, 0.96, 0.93]\n",
        "Mini_bear = [0.57, 0.69, 0.68, 0.84, 0.85]\n",
        "LSTM_side = [0.945, 0.948, 0.964, 0.982, 0.954]\n",
        "Mini_side = [0.603, 0.576, 0.587, 0.673, 0.51]\n",
        "LSTM_bull = [0.066, 0, 0.178, 0.848, 0.916]\n",
        "Mini_bull = [0.008, 0.753, 0.655, 0.766, 0.839]"
      ],
      "metadata": {
        "id": "ZrTMpMUavjCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(LSTM_bear, color='skyblue', label='LSTM_bear')\n",
        "ax.plot(Mini_bear, color='red', label='Mini_bear')\n",
        "ax.plot(LSTM_side, color='blue', label='LSTM_side')\n",
        "ax.plot(Mini_side, color='salmon', label='Mini_side')\n",
        "ax.plot(LSTM_bull, color='royalblue', label='LSTM_bull')\n",
        "ax.plot(Mini_bull, color='tomato', label='Mini_bull')\n",
        "\n",
        "# title legend\n",
        "ax.set_title('Cohen Kappa Scores For Each Model')\n",
        "ax.legend()\n",
        "\n",
        "# plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "80hrZvsCv7DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tU1RPhPTxXap"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AtSJV_2G44Wl",
        "JIInNQA5My4f",
        "Q-G4CBu63dJk",
        "BLULJxkbTFXu",
        "n08r2j4TgS3F",
        "GzGBYtX5M9pC",
        "giVBXEx7NM-B",
        "u4OGP_PANb7i",
        "7cLNMrRaNkyv",
        "-WCzSnoHNvHq",
        "XGvYlgM5N0T9",
        "4tZOpdJTBYYw",
        "8zmTAT2eJkKe",
        "07yCLD3PJNGY",
        "Jmt5jnzo4yN0",
        "jfXccf4N7N15",
        "sF0EF---ETcb",
        "PAewyn0OgowG",
        "yhspU0igFlKF",
        "JlQ0WDzwFpvh",
        "fp7MIyiyNjBA",
        "5yfIe3ev5BKA",
        "qWJ-mzPlDf95",
        "HeSz1NTSDmRh",
        "QapJuaLLDv0j",
        "r2cFo0wrBL4a"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}